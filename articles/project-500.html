<!DOCTYPE html>
<html lang="en">
<head>

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-129723079-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-129723079-1');
  </script>


  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">
  <title>Case study: 500 TPS</title>
  <meta name="description" content="">
  <meta name="author" content="">

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link href="//fonts.googleapis.com/css?family=Raleway:400,300,600" rel="stylesheet" type="text/css">

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="stylesheet" href="/static/css/normalize.css">
  <link rel="stylesheet" href="/static/css/skeleton.css">
  <link rel="stylesheet" href="/static/css/main.css">
  <link rel="stylesheet" href="/css/layout.css">
  
  <link rel="stylesheet" type="text/css" href="/static/css/prism.css"/> 
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  
  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="images/favicon.png">

</head>
<body>

  <!-- Primary Page Layout
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <div class="container">
    <div class="row books">
        <div class="twelve columns header">
			<ul>
			<li><a
            href="https://www.amazon.com/Perl-Hacks-Programming-Debugging-Surviving/dp/0596526741/"
            target="_blank"><img src="/static/images/perl-hacks.jpg"
            alt="The cover of the 'Perl Hacks' book" class="book"></a></li>
			<li><a
            href="https://www.amazon.com/Beginning-Perl-Curtis-Poe/dp/1118013840/"
            target="_blank"><img src="/static/images/beginning-perl.jpg"
            alt="The cover of the 'Beginning Perl' book" class="book"></a></li>
			<li><img class="book" src="/static/images/profile.png" alt="An
            image of Curtis Poe, holding some electronic equipment in front of
            his face."></li>
			</ul>
        </div>
    </div>
    <div class="row">
      <div class="three columns">
        <h1>Links</h1>
        <ul>
          <li><a href="/index.html">Follow Me</a></li>
          <li><a href="/articles.html">Technical Articles</a></li>
          <li><a href="/publicspeaking.html">Public Speaking</a></li>
          <li><a href="/hireme.html">Hire Me</a></li>
          <li><a href="/blog.html">Blog</a></li>
          <li><a href="/wildagile.html">WildAgile</a></li>
          <li><a href="/tau-station.html">Tau Station</a></li>
          <li><a href="/starmap.html">Starmap</a></li>
        </ul>
      </div>

        <div class="nine columns verticalLine">
            <h1>Case study: 500 TPS</h1>
            <hr>


<nav role="navigation" class="table-of-contents">
    <ul>
    <li class="indent-1"><a href="#the-constraints">The Constraints</a></li>
    <li class="indent-1"><a href="#hitting-the-ground">Hitting the Ground</a></li>
    <li class="indent-2"><a href="#the-tyranny-of-orms">The Tyranny of ORMs</a></li>
    <li class="indent-2"><a href="#caching">Caching</a></li>
    <li class="indent-2"><a href="#soap">SOAP</a></li>
    <li class="indent-2"><a href="#tax-server">Tax Server</a></li>
    <li class="indent-2"><a href="#the-big-test">The Big Test</a></li>
    <li class="indent-1"><a href="#what-we-delivered">What We Delivered</a></li>
    <li class="indent-2"><a href="#overview">Overview</a></li>
    <li class="indent-2"><a href="#solutions-implemented">Solutions Implemented</a></li>
    <li class="indent-3"><a href="#new-caching-architecture">New Caching Architecture</a></li>
    <li class="indent-3"><a href="#data-storage-performance">Data Storage Performance</a></li>
    <li class="indent-3"><a href="#implement-staged-processing">Implement Staged Processing</a></li>
    <li class="indent-3"><a href="#process-streamlining">Process Streamlining</a></li>
    <li class="indent-3"><a href="#repeatable">Repeatable</a></li>
    <li class="indent-1"><a href="#caveat">Caveat</a></li>
    </ul>
</nav>
<hr>

<blockquote>The Devil went down to Georgia.<br>
He was lookin' for a soul to steal.<br>
He was in a bind 'cause he was way behind.<br>
He was willing to make a deal.
<div class="attribution">"Devil Went Down To Georgia"—The Charlie Daniels Band</div>
</blockquote>


<p>Our client had just won a nice contract but were in a corner: their legacy
system, while powerful, was slow. They could not process more than 39 credit
card transactions per second. They needed to get to 500 transactions per
second for an online event lasting for 30 minutes. They had two weeks to get a
proof of concept running.</p>

<p>They turned to our company, <a href="https://allaroundtheworld.fr" target="_blank">All Around the World</a> <span class="fa fa-external-link fa_custom"></span>, because we have a proven track record with them.</p>

<p>We had a senior systems architect, Shawn, and a senior software architect,
Noel, on the project. Our managing director, Leïla, oversaw the project and
ensured that if we had questions, she had the answers. I was brought in
because there was simply too much work to do in two weeks.  Fortunately,
though I didn't know the project, I had Shawn and Noel who knew the system
well to help get me up to speed, and Leïla to answer business questions.</p>

<p>We had two weeks to improve their performance by an order of magnitude.</p>

<h1><a name="the-constraints"></a>The Constraints</h1>

<p>There were several key constraints we had to consider. Our overriding
consideration was ensuring that PCI-compliance was strictly adhered to to
ensure that customer data was always protected. Second, the client had
developed an in-house ORM (object-relational mapper) many years ago and like
all projects, it grew tremendously. While it was powerful, it was extremely
slow and had a lot of business logic embedded in it. Third, because we only
had two weeks for the first pass, we were given permission to take
"shortcuts", where necessary, with the understanding that all work was to be
thoroughly documented and tested, and easy to either merge or remove, as
needed.</p>

<p>Finally, we could change anything we wanted so long as we didn't change the
API, or cause any breaking changes anywhere else in the code. There was no
time to give customers a "heads up" that they would need to make changes.</p>

<h1><a name="hitting-the-ground"></a>Hitting the Ground</h1>

<p>Because the event only lasted 30 minutes, whatever solution we implemented
didn't have to stay up for long. This also meant that whatever solution we
implemented had to be disabled quickly if needed. We also knew that only a few
customers would use this new "fast" solution, and only one payment
provider needed to be supported.</p>

<p>Noel immediately started tracing the full code path through the system,
taking copious notes about any behavior we would need to be aware of. Shawn
was investigating the databases, the servers, the network architecture, and
assessing what additional resources could be brought online and tested in two
weeks.</p>

<p>I, being new to the project, started by taking a full-stack integration
test representing one of these transactions and studied it to learn the
code and its behavior. In particular, I wanted to better understand the
database behavior as this is often one of the most significant
bottlenecks.</p>

<p>I was particularly concerned because the client was in the midst of a large
migration from Oracle to PostgreSQL, so making changes at the database level
was not an option. We needed to know immediately if this was one of the
bottlenecks. I wrote code which would dump out a quick summary of database
activity for a block of code.  It looked sort of like this:</p>

<pre><code class="language-perl">explain dbitrace(
	name =&gt; 'Full stack transaction request',
	code =&gt; sub { $object-&gt;make_full_request },
    save =&gt; '/tmp/all_sql.txt',
);
</code></pre>

<p>The summary output looked similar to this:</p>

<pre><code class="language-perl">{
  report =&gt; {
    name =&gt; 'Full stack transaction request',
    time =&gt; '3.70091 wallclock secs'
  },
  sql =&gt; {
    delete =&gt; 1,
    insert =&gt; 7,
    select =&gt; 137,
    update =&gt; 32,
    total  =&gt; 177,
  }
}</code></pre>

<p>We had a problem. Even our client was surprised about the amount of
database activity for a single "buy this thing" request.</p>

<p>None of the database activity was particularly slow, but there was a lot of
it. Deadlocks weren't an issue given that the changed data was for the current
request. But the ORM was killing us.</p>

<h2><a name="the-tyranny-of-orms"></a>The Tyranny of ORMs</h2>

<p>I love working with a good ORM, but ORMs generally trade execution
performance for developer performance. You have to decide which is more
important to you. Further, in two decades of working with ORMs, I have only
once seen an in-house ORM which was on-par with, or superior to, commercial or
open source products. And it was an ORM optimized for reporting, something
many ORMs struggle with.</p>

<p>For this particular ORM, every time a request was made, it would assemble a
bunch of metadata, check permissions, make decisions based on whether or not
it was using Oracle or PostgreSQL, check to see if the data was cached, and
<em>then</em> check to see if the data was in the database. Instantiating
every object was very slow, even if there was no data available. And the code
was instantiating (and throwing away without using) hundreds of these
objects.</p>

<p>We considered using a "pre-flight" check to see if the data was there
before instantiating the objects, but there was so much business logic
embedded in the ORM layer that this was not a practical solution. And we
couldn't simply fetch the data directly because, again, the ORM had too much
business logic. We had to reduce the calls to the database.</p>

<p>After an exhaustive analysis, we found out several things.</p>

<ul>
  <li>Some of the calls were for a "dead" part of the system that no one
  really remembered, but was clearly unused.</li>
  <li>Numerous duplicate calls were being made to <em>unchanging</em> data. We
  could cache those objects safely.</li>
  <li>A number of calls were being made for important data that wasn't
  relevant to our code path, so we could skip them.</li>
</ul>

<p>Our first step at addressing the problem was to ensure that everything was
wrapped in configuration variables that allowed us to easily turn on or off
different code paths for our project. Fortunately, the client had a
system that allowed them to update the configuration data without restarting
the application servers, so this made our choice much safer.</p>

<p>Once that was in place, our first pass cut our SQL calls roughly in half,
tremendously sped up the code, and the tests still passed. But we were nowhere
near 500 transaction per second.</p>

<h2><a name="caching"></a>Caching</h2>

<p>It's often said that the three most common bugs in software are related to
caching and off-by-one errors. And we had caching problems in spades.</p>

<p>We couldn't afford cache misses during the event, so we needed to "preheat"
the caches by ensuring all information was loaded on server startup. Except
that we had a few problems with this.</p>

<p>First, it was <em>slow</em> because there was a lot of data to cache.
Second, it consumed a lot of memory and we were concerned about memory
contention issues. Third, the app caches primarily used per-server memcached
instances, but the data was global, not per-server, thus causing a lot of
unnecessary duplication. Shawn had already known about much of this, so one of
the first things he did was set up a secure Redis cluster with failover to
replace memcached, where appropriate. We only had to heat the cache once,
bringing up servers was faster, and reducing per-server memory
consumption.</p>

<p>The application itself also used heavy in-memory caching (implemented as
hashes), which we replaced with pre-loaded shared cache entries, thereby
lowering memory requirements even further.</p>

<p>As part of this work, we centralized all caching knowledge into a single
namespace rather than the ad-hoc "per module" implementations. We also created
a single configuration file to control all of it, making caching much simpler
for our client. This is one of the many features they still use today.</p>

<h2><a name="soap"></a>SOAP</h2>

<p>Another serious bottleneck was their SOAP server.  Our client made good use
of WSDL (Web Services Description Language) to help their customers understand
how to create SOAP requests and SOAP was integral to their entire pipeline.
The client would receive a SOAP request, parse it, extract the necessary data,
process that data, create <em>another</em> SOAP request, pass this to backend
servers, which would repeat the process with the new SOAP request.</p>

<p>SOAP stands for "Simple Object Access Protocol." But SOAP isn't "objects",
and it's not "simple." It's a huge XML document, with an outer envelope
telling you how to parse the message contents. Reading and writing SOAP is
<em>slow</em>.  Further, our client's SOAP implementation had grown over the
years, with each new version of their SOAP interface starting with
cutting-and-pasting the last version's code into a new module and modifying
that. There were, at the time of the project, 24 versions,<sup id="1-return"><a href="#1">1</a></sup> most of which had heavily duplicated code. To optimize
their SOAP, we were facing a problem. However, we dug in further and found
that only one version would be used for this project, so our client authorized
us to skip updating the other SOAP versions.</p>

<p>We tried several approaches to fine-tuning the SOAP, including replacing
many <code class="language-perl">AUTOLOAD</code> (dynamically generated)
methods with static ones. In Perl, <code class="language-perl">AUTOLOAD</code>
methods are optional "fallback" methods that are used when Perl cannot find a
method of the desired name. However, this means Perl must carefully walk
through inheritance heirarchies to ensure the method requested isn't there
before falling back to <code class="language-perl">AUTOLOAD</code>. This can
add considerable overhead to a request. For this and other reasons, the use of
<code class="language-perl">AUTOLOAD</code> is heavily discouraged.</p>

<p>Replacing these methods was extremely difficult work because the <code
class="language-perl">AUTOLOAD</code> methods were used heavily, often calling
each other, and  had grown tremendously over the years. Many of them were
extremely dangerous to pick apart due to their very complex logic. We managed
to shave some more time with this approach but stopped before we tackled the
most complicated ones. There was only so much risk we were willing to
take.</p>

<p>Noel, in carefully reading through the SOAP code, also found several code
paths that were slow, but didn't apply to our requests. Unfortunately, we
could not simply skip them because the SOAP code was tightly coupled with
external business logic. Skipping these code paths would invariably break
something else in the codebase. What he proposed, with caution, is the
creation of a special read-only "request metadata" singleton. Different parts
of the code, when recognizing they were in "web context", could request this
metadata and skip non-essential codepaths. While singletons are frowned upon
by experienced developers, we were under time pressure and in this case, our
SOAP code and the code it called could all consult the singleton to coordinate
their activity.</p>

<p>The singleton is one of the many area where deadlines and reality collide.
Hence, the "Devil Went Down to Georgia" quote at the beginning of this case
study. We were not happy with this solution, but sometimes you need to
sacrifice "best practices" when you're in an emergency situation. We alerted
our client to the concern so they could be aware that this was not a long-term
solution.</p>

<p>We also leveraged that singleton to allow us to alert the back-end that
encrypted transaction data was available directly via Redis. The backend
merely needed to decrypt that data without the need to deserialize SOAP and
then decrypt the data. Another great performance improvement. Sadly, like many
others improvements, it was specific to this one customer and this one payment
provider.</p>

<h2><a name="tax-server"></a>Tax Server</h2>

<p>By this time, with the above and numerous other fixes in place, we were
ready to do our initial QA runs. We didn't expect to hit our 500 TPS target,
but we were pretty confident we had achieved some major speed gains. And, in
fact, we did have some pretty impressive speed gains, except that periodically
some of our requests would simply halt for several seconds. Due to the
complicated nature of the system, it wasn't immediately clear what was going
on, but we finally tracked this down to the tax server.</p>

<p>Our client was processing credit card transactions and for many of them,
taxes had to be applied. Calculating taxes is complicated enough that there
are companies that provide "tax calculation as a service" and our client was
using one of them. Though the tax service assured us they could handle the
amount of traffic we were sending, they would periodically block. It wasn't
clear if this was a limitation of their test servers that we would hit with
their production servers, but we could not take this chance.</p>

<p>We tried several approaches, including caching of tax rates, but the wide
diversity of data we relied on to calculate tax meant a high cache miss rate,
not allowing us to solve the problem. Finally, one of our client's developers
who knew the tax system fairly well came up with a great solution.  He
convinced the tax service to provide a "one-time" data dump of tax rates that
would not be valid long, but would be valid long enough for our short event.
We could load the data into memory and read it directly and skip the tax
server entirely. Though the resulting code was complicated, it worked, and our
requests no longer blocked.</p>

<h2><a name="the-big-test"></a>The Big Test</h2>

<p>With the above, and many other optimizations, we felt confident in what we
delivered and in a late-night test run, our client was ecstatic. We were
handling 700 transactions per second, almost twenty times faster than when we
started. It was time for our first production run, using beefier servers and a
much faster network.</p>

<p>And that, to put it mildly, was disappointing. We were only running about
200 to 300 transactions per second. As you may recall, the client had been
migrating from Oracle to PostgreSQL. The QA servers were far less powerful
than the production servers, but they were running PostgreSQL. The production
servers were running Oracle. Oracle is the fast, beastly sports car that will
quickly outpace that Honda Accord you're driving ... so long as the sports car
has an expert driver, regular maintenance, and a highly trained pit crew to
look after it.</p>

<p>Out of the box, PostgreSQL just tends to be fast. Often companies find that
a single PostgreSQL DBA working with competent developers is enough to easily
handle their traffic. But we had Oracle. And it wasn't going away before the
deadline. Our first two weeks was up and our proof of concept worked, but
greatly exceeding client requirements still wasn't fast enough.</p>

<p>And that's when the client informed us that they had a bit more than two
weeks, so we had more time. It was going to be more nights and weekends for
us, but we weren't about to give up now.</p>

<p>Oracle wasn't the only problem, as it turns out. This faster, higher
capacity network had switches that automatically throttled traffic surges.
This was harder to diagnose and work around, but alleviated the networking
issues.</p>

<p>With the use of Oracle being a bottleneck, we had to figure out a way to
remove those final few SQL statements. Our client suggested we might be able to
do something unusual and it's based on how credit cards work.</p>

<p>When you check into a hotel and reserve your room with a credit card, your
card is not charged for the room. Instead, they "authorize" (sometimes
called preauthorization) the charge. That lowers the credit limit on your card
and reserves the money for the hotel. They will often preauthorize for over
the amount of your bill, to allow a small wiggle room for incidental charges,
such as food you may have ordered in the hotel.</p>

<p>Later, when you check out, they'll charge your card. Or, if you leave and
forget to check out, they have a preauthorization, so they can still "capture"
the preauthorization and get the money deposited to their account. This
protects them from fraud, or guests who are simply forgetful.</p>

<p>In our case, the client often used preauthorization for payments and after
the preauthorization was successful, the card would be charged. This had a few
internal benefits for them that we won't go into now.</p>

<p>What the client pointed out is that the money was reserved, so we had
several days before we needed to charge the card. When the back-end sent the
preauthorization to the front-end, the front-end would then submit the charge.
But we didn't have to do that. Instead, we were able to serialize the charge
request and store it in an encrypted file on the front-end server (with the
decryption key unavailable on those servers). Later, a utility we wrote read
and batch processed those files <em>after</em> the event, allowing us to
finish the transaction processing asynchronously.</p>

<p>This worked very well and by the time we were done, our changes to the code
resulted in a single SQL <code>SELECT</code> statement. Further, after this
and a few other tweaks, we were easily able to hit 500 transactions per second
on their production system. When the event happened shortly after, the code
ran flawlessly.</p>

<h1><a name="what-we-delivered"></a>What We Delivered</h1>

<h2><a name="overview"></a>Overview</h2>

<p>Increase the performance of Business Transactions from 39/sec to 500+/sec.</p>

<h2><a name="solutions-implemented"></a>Solutions Implemented</h2>

<p>The overall implemented solution could not change the outward facing API calls
as end users of the service were not to be impacted by backend changes. The
solution could also not effect the traditional processing as clients would
be issuing both traditional and streamlined transactions.</p>

<h3><a name="new-caching-architecture"></a>New Caching Architecture</h3>

<p>Implement a new redundant shared caching architecture using Redis as a data
store, to replace the existing on application server memcached instances.</p>

<p>Update existing application code to remove the use of internal application
caches (implemented as hashes), and replace with pre-loaded shared cache
entries.</p>

<p>The new caching system caches items using both the legacy and the new
caching system, so that production systems can be transitioned to the unified
caching system without downtime.</p>

<p>Allowed a cache storage backend to define its storage scope, so that caches
can be managed based on the storage locality:</p>

   - "process": cache is stored in local per-process memory
   - "server": cache is stored on the local server, and shared by all processes on the server
   - "global": cache is stored globally, and shared by all processes on any server
 - logs basic caching statistics for each cache namespace when the process exits

<h3><a name="data-storage-performance"></a>Data Storage Performance</h3>

<p>Create an additional data storage path that replaces storing intermediate data
in the backend shared database with the shared caching architecture.</p>

<p>Remove the use of a third party provider API call, and replace with a static
table lookup.</p>

<h3><a name="implement-staged-processing"></a>Implement Staged Processing</h3>

<p>As data processing in real time is not a requirement, add a data path that
allows for separation of the front end API responses from the third party
backend processing. This allowed for heavy backend processes to be deferred and
processed in smaller batches.</p>

<h3><a name="process-streamlining"></a>Process Streamlining</h3>

<p>Worked closely with the client to determine which parts of the process are
required during the processing, and which can be bypassed to save overhead.</p>

<p>Code review and clean up. Refactor code base to remove extraneous or
ill-performing legacy code.</p>

<h3><a name="repeatable"></a>Repeatable</h3>

<p>The process was re-engineered to be configurable allowing the same pipeline to
be reimplemented for feature events/customer requirement through simple
documented configuration options, while not impacting traditional processing.</p>

<h1><a name="caveat"></a>Caveat</h1>

<p>This information was compiled from our notes from this project and not
written while the project was ongoing. Some details have undoubtedly been
forgotten.</p>


    <hr>

    <p><strong>Footnotes</strong></p>

    <div class="footnotes">
<p id="1"><a href="#1-return">[1]</a> Many of the oldest SOAP versions had long been deprecated,
so that helped.</p>
    </div>
          <hr>
          <p>If you'd like top-notch consulting or training, <a
          href="mailto:curtis.poe@gmail.com">email me</a> and let's discuss
          how I can help you. Read my <a href="/hireme.html">hire me</a> page
          to learn more about my background.</p>
        </div>
    </div>
<!-- End Document
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
    <div class="row">
      <div class="three columns">
        <p></p>
      </div>
      <div class="nine columns">
        <hr>
        <p>Copyright &copy; 2018-2019 by Curtis "Ovid" Poe.</p>
      </div>
    </div>
        <div id="disqus_thread"></div>
    <div class="row">
      <div class="twelve columns">
      
        <script>
        var disqus_config = function () {
            this.page.url        = "https://ovid.github.io/articles/project-500.html";
            this.page.identifier = "articles/project-500";
            this.page.title      = "Case study: 500 TPS";
        };
        (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://https-ovid-github-io.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
        })();
        </script>
        <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
      
        </div>
    </div>
    </div>

<script src="/static/js/prism.js"></script>

</body>
</html>

